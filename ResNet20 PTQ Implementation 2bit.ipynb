{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8d1eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#Pytorch Quantization\n",
    "import torch.quantization\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Model Import\n",
    "from ResNet20 import resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d5e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = dict(\n",
    "    seed = 42,\n",
    "    train_batch_size = 128,\n",
    "    valid_batch_size = 256,\n",
    "    num_calibration_batches = 32,\n",
    "    num_classes = 10,\n",
    "    device = torch.device(\"cpu\"),\n",
    "    bits = 2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3c275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATHS = 'ResNet20 final.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2628f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8a01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, labels):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2adc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_fn(model, dataloader, device, neval_batches):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    PREDS = []\n",
    "    count = 0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader), ncols=100)\n",
    "    for step, data in bar:        \n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "            \n",
    "        # рачсет вывода\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        _, preds = output.max(1)\n",
    "        correct += preds.eq(targets).sum()\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        sum_loss = running_loss / dataset_size\n",
    "        \n",
    "        sum_score = correct.cpu().detach().numpy() / dataset_size\n",
    "        \n",
    "        bar.set_postfix({'Valid_Loss':sum_loss, 'Valid_Score':sum_score})\n",
    "        PREDS.append(output.view(-1).cpu().detach().numpy()) \n",
    "        if count >= neval_batches:\n",
    "            PREDS = np.concatenate(PREDS)\n",
    "            return sum_loss, sum_score, PREDS\n",
    "        \n",
    "    PREDS = np.concatenate(PREDS)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sum_loss, sum_score, PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2930bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_symmetric_quantizer(x, bits=8.0, minv=None, maxv=None, signed=True, \n",
    "                                scale_bits=0.0, num_levels=None, scale=None, simulated=True):\n",
    "    if minv is None:\n",
    "        maxv = torch.max(torch.abs(x))\n",
    "        minv = - maxv if signed else 0\n",
    "\n",
    "    if signed:\n",
    "        maxv = np.max([-float(minv), float(maxv)])\n",
    "        minv = - maxv \n",
    "    else:\n",
    "        minv = 0\n",
    "    \n",
    "    if num_levels is None:\n",
    "        num_levels = 2 ** bits\n",
    "\n",
    "    if scale is None:\n",
    "        scale = (maxv - minv) / (num_levels - 1)\n",
    "\n",
    "    if scale_bits > 0:\n",
    "        scale_levels = 2 ** scale_bits\n",
    "        scale = torch.round(torch.mul(scale, scale_levels)) / scale_levels\n",
    "            \n",
    "    ## clamp\n",
    "    x = torch.clamp(x, min=float(minv), max=float(maxv))\n",
    "        \n",
    "    x_int = torch.round(x / scale)\n",
    "    \n",
    "    if signed:\n",
    "        x_quant = torch.clamp(x_int, min=-num_levels/2, max=num_levels/2 - 1)\n",
    "        assert(minv == - maxv)\n",
    "    else:\n",
    "        x_quant = torch.clamp(x_int, min=0, max=num_levels - 1)\n",
    "        assert(minv == 0 and maxv > 0)\n",
    "        \n",
    "    x_dequant = x_quant * scale\n",
    "    \n",
    "    return x_dequant if simulated else x_quant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23261b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_weights(w):\n",
    "    '''\n",
    "    Квантизация весов слоя \n",
    "    '''\n",
    "    \n",
    "    # uniform symmetric quantization\n",
    "    qw = uniform_symmetric_quantizer(w, bits=CONFIG['bits'])\n",
    "\n",
    "    err = float(torch.sum(torch.mul(qw - w, qw - w)))\n",
    "\n",
    "    return qw, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da267e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_checkpoint(checkpoint):\n",
    "    '''\n",
    "    Квантизация слоев\n",
    "    '''\n",
    "    bits = CONFIG['bits']\n",
    "\n",
    "    print('quantizing weights into %s bits, %s layers' % (bits, len(checkpoint.keys())))\n",
    "\n",
    "    all_quant_error, all_quant_num = 0, 0\n",
    "    for each_layer in checkpoint.keys():\n",
    "        \n",
    "        if '.num_batches_tracked' in each_layer or '.minv' in each_layer or '.maxv' in each_layer or 'bn' in each_layer or '.downsample' in each_layer or 'fc.bias' in each_layer :\n",
    "            continue\n",
    "        \n",
    "        each_layer_weights = checkpoint[each_layer].clone()\n",
    "\n",
    "        print('quantize for: %s, size: %s' % (each_layer, each_layer_weights.size()))\n",
    "        print('weights range: (%.4f, %.4f)' % \n",
    "                            (torch.min(each_layer_weights), torch.max(each_layer_weights)))\n",
    "\n",
    "        quant_error, quant_num = 0, 0\n",
    "        output_channel_num = each_layer_weights.size()[0]\n",
    "        # channel-wise quant for each output channel\n",
    "        for c in range(output_channel_num):  \n",
    "            w = each_layer_weights[c, :].clone()\n",
    "            #w = each_layer_weights.clone()\n",
    "            qw, err = quant_weights(w)\n",
    "\n",
    "            each_layer_weights[c, :] = qw\n",
    "            #each_layer_weights = qw\n",
    "            quant_error += err\n",
    "            quant_num += len(qw.reshape(-1, 1))\n",
    "\n",
    "        all_quant_num += quant_num\n",
    "        all_quant_error += quant_error\n",
    "\n",
    "        checkpoint[each_layer] = each_layer_weights\n",
    "        print('layer quant RMSE: %.4e' % np.sqrt(quant_error / quant_num))\n",
    "        \n",
    "    rmse = np.sqrt(all_quant_error / all_quant_num)\n",
    "    print('\\ntotal quant RMSE: %.4e' % rmse)\n",
    "\n",
    "    return checkpoint, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1cee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantActivations(nn.Module):\n",
    "    '''\n",
    "    Квантизация активаций:\n",
    "    (1) the input of conv layer\n",
    "    (2) the input of linear fc layer\n",
    "    (3) the input of pooling layer\n",
    "    '''\n",
    "    def __init__(self, act_bits, get_stats, minv=None, maxv=None, \n",
    "        calibrate_sample_size=512, calibrate_batch_size=4, topk=10):\n",
    "        '''\n",
    "        calibrate_sample_size: calibration sample size, typically from random training data\n",
    "        calibrate_batch_size: calibration sampling batch size\n",
    "        topk: calibrate topk lower and upper bounds\n",
    "        '''\n",
    "        super(QuantActivations, self).__init__()\n",
    "        self.act_bits = act_bits\n",
    "        self.get_stats = get_stats\n",
    "        self.index = 0\n",
    "        self.topk = topk\n",
    "        self.sample_batches = calibrate_sample_size // calibrate_batch_size\n",
    "        stats_size = (self.sample_batches, self.topk) if self.get_stats else 1\n",
    "        \n",
    "        self.register_buffer('minv', torch.zeros(stats_size))\n",
    "        self.register_buffer('maxv', torch.zeros(stats_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.get_stats:\n",
    "            y = x.clone()\n",
    "            y = torch.reshape(y, (-1,))\n",
    "            y, indices = torch.sort(y)\n",
    "            topk_mins = y[:self.topk]\n",
    "            topk_maxs = y[-self.topk:]\n",
    "            if self.index < self.sample_batches:\n",
    "                self.minv[self.index, :] = topk_mins\n",
    "                self.maxv[self.index, :] = topk_maxs\n",
    "                self.index += 1\n",
    "\n",
    "        if self.act_bits > 0:\n",
    "            ## uniform quantization\n",
    "            if self.minv is not None:\n",
    "                if self.minv >= 0.0: # activation after relu\n",
    "                    self.minv *= 0.0\n",
    "                    self.signed = False\n",
    "                else: \n",
    "                    self.maxv = max(-self.minv, self.maxv) \n",
    "                    self.minv = - self.maxv\n",
    "                    self.signed = True\n",
    "            x = uniform_symmetric_quantizer(x, bits=self.act_bits, \n",
    "                    minv=self.minv, maxv=self.maxv, signed=self.signed)\n",
    "        return x\n",
    "\n",
    "\n",
    "def quant_model_acts(model, act_bits, get_stats, calibrate_batch_size=4):\n",
    "    \"\"\"\n",
    "    Добавление активаций к слоям\n",
    "    \"\"\"\n",
    "    if type(model) in [nn.Conv2d, nn.Linear, nn.AdaptiveAvgPool2d]:\n",
    "        quant_act = QuantActivations(act_bits, get_stats, calibrate_batch_size=calibrate_batch_size)\n",
    "        return nn.Sequential(quant_act, model)\n",
    "    elif type(model) == nn.Sequential:\n",
    "        modules = []\n",
    "        for name, module in model.named_children():\n",
    "            modules.append(quant_model_acts(module, act_bits, get_stats, calibrate_batch_size=calibrate_batch_size))\n",
    "        return nn.Sequential(*modules)\n",
    "    else:\n",
    "        quantized_model = copy.deepcopy(model)\n",
    "        for attribute in dir(model):\n",
    "            module = getattr(model, attribute)\n",
    "            if isinstance(module, nn.Module):\n",
    "                setattr(quantized_model, attribute, \n",
    "                    quant_model_acts(module, act_bits, get_stats, calibrate_batch_size=calibrate_batch_size))\n",
    "        return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee1c9250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def save_model_act_stats(model, save_path):\\n    checkpoint = model.state_dict()\\n    act_stats = copy.deepcopy(checkpoint)\\n    for key in checkpoint:\\n        if '.minv' not in key and '.maxv' not in key:\\n            del act_stats[key]\\n    torch.save(act_stats, save_path)\\n    return act_stats\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def save_model_act_stats(model, save_path):\n",
    "    checkpoint = model.state_dict()\n",
    "    act_stats = copy.deepcopy(checkpoint)\n",
    "    for key in checkpoint:\n",
    "        if '.minv' not in key and '.maxv' not in key:\n",
    "            del act_stats[key]\n",
    "    torch.save(act_stats, save_path)\n",
    "    return act_stats'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb888d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"quantized_model1 = resnet20()\\nquantized_model1.to(CONFIG['device'])\\nquantized_model1.load_state_dict(torch.load(MODEL_PATHS))\\ncheckpoint = quantized_model1.state_dict()\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''quantized_model1 = resnet20()\n",
    "quantized_model1.to(CONFIG['device'])\n",
    "quantized_model1.load_state_dict(torch.load(MODEL_PATHS))\n",
    "checkpoint = quantized_model1.state_dict()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71b3a8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#get activation stats\\nquantized_model1 = quant_model_acts(quantized_model1, 0, True, CONFIG['num_calibration_batches'])\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#get activation stats\n",
    "quantized_model1 = quant_model_acts(quantized_model1, 0, True, CONFIG['num_calibration_batches'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf4ba09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_loader = torch.utils.data.DataLoader(\\n        torchvision.datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\\n            transforms.Pad(4),\\n            transforms.RandomCrop(32),\\n            transforms.RandomHorizontalFlip(),\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n        ]), download=True),\\n        batch_size=CONFIG['train_batch_size'], shuffle=True,\\n        num_workers=2)\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.Pad(4),\n",
    "            transforms.RandomCrop(32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]), download=True),\n",
    "        batch_size=CONFIG['train_batch_size'], shuffle=True,\n",
    "        num_workers=2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "656bb003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"valid_fn(quantized_model1, train_loader, CONFIG['device'], CONFIG['num_calibration_batches'])\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''valid_fn(quantized_model1, train_loader, CONFIG['device'], CONFIG['num_calibration_batches'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "482a8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_stats_save_path = 'stats/%s_act_stats.pth' % \"ResNet20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1019f21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# save the activation stats\\nos.makedirs('stats/', exist_ok=True)\\nsave_model_act_stats(quantized_model1, act_stats_save_path)\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# save the activation stats\n",
    "os.makedirs('stats/', exist_ok=True)\n",
    "save_model_act_stats(quantized_model1, act_stats_save_path)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86aaff",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "512cd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_clip_bounds(stats, act_clip_method, min_or_max):\n",
    "    if act_clip_method.startswith('top'):\n",
    "        topk = int(act_clip_method.split('_')[1])\n",
    "        assert(topk <= 20)\n",
    "        stats = stats[:, :topk] if min_or_max == 'min' else stats[:, -topk:]\n",
    "        values, indices = torch.median(stats, 1)\n",
    "        return torch.mean(values)\n",
    "    else:\n",
    "        raise RuntimeError(\"Please implement for activation clip method: %s !!!\" % act_clip_method) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3df22597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_act_stats(model, load_path, act_clip_method):\n",
    "    checkpoint = model.state_dict()\n",
    "    act_stats = torch.load(load_path)\n",
    "    for key in act_stats:\n",
    "        min_or_max = 'min' if '.minv' in key else 'max'\n",
    "        value = act_clip_bounds(act_stats[key], act_clip_method, min_or_max)\n",
    "        key = key.replace('module.', '')\n",
    "        checkpoint[key][0] = value\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "827ee370",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model2 = resnet20()\n",
    "quantized_model2.to(CONFIG['device'])\n",
    "quantized_model2.load_state_dict(torch.load(MODEL_PATHS))\n",
    "checkpoint = quantized_model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7bf7e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantizing weights into 2.0 bits, 128 layers\n",
      "quantize for: conv.weight, size: torch.Size([16, 3, 3, 3])\n",
      "weights range: (-1.7392, 2.0936)\n",
      "layer quant RMSE: 2.2709e-01\n",
      "quantize for: layer1.0.conv1.weight, size: torch.Size([16, 16, 3, 3])\n",
      "weights range: (-0.9321, 1.2331)\n",
      "layer quant RMSE: 1.1933e-01\n",
      "quantize for: layer1.0.conv2.weight, size: torch.Size([16, 16, 3, 3])\n",
      "weights range: (-0.8008, 0.9115)\n",
      "layer quant RMSE: 1.1508e-01\n",
      "quantize for: layer1.1.conv1.weight, size: torch.Size([16, 16, 3, 3])\n",
      "weights range: (-0.7769, 0.7389)\n",
      "layer quant RMSE: 1.0306e-01\n",
      "quantize for: layer1.1.conv2.weight, size: torch.Size([16, 16, 3, 3])\n",
      "weights range: (-0.7376, 0.6565)\n",
      "layer quant RMSE: 9.2983e-02\n",
      "quantize for: layer1.2.conv1.weight, size: torch.Size([16, 16, 3, 3])\n",
      "weights range: (-1.0651, 0.7439)\n",
      "layer quant RMSE: 1.1081e-01\n",
      "quantize for: layer1.2.conv2.weight, size: torch.Size([16, 16, 3, 3])\n",
      "weights range: (-0.7821, 0.9201)\n",
      "layer quant RMSE: 9.5026e-02\n",
      "quantize for: layer2.0.conv1.weight, size: torch.Size([32, 16, 3, 3])\n",
      "weights range: (-0.5892, 0.5763)\n",
      "layer quant RMSE: 8.2448e-02\n",
      "quantize for: layer2.0.conv2.weight, size: torch.Size([32, 32, 3, 3])\n",
      "weights range: (-0.4570, 0.6083)\n",
      "layer quant RMSE: 7.1623e-02\n",
      "quantize for: layer2.1.conv1.weight, size: torch.Size([32, 32, 3, 3])\n",
      "weights range: (-0.6653, 0.6111)\n",
      "layer quant RMSE: 7.8590e-02\n",
      "quantize for: layer2.1.conv2.weight, size: torch.Size([32, 32, 3, 3])\n",
      "weights range: (-0.5644, 0.5128)\n",
      "layer quant RMSE: 7.0666e-02\n",
      "quantize for: layer2.2.conv1.weight, size: torch.Size([32, 32, 3, 3])\n",
      "weights range: (-0.4668, 0.7200)\n",
      "layer quant RMSE: 7.8282e-02\n",
      "quantize for: layer2.2.conv2.weight, size: torch.Size([32, 32, 3, 3])\n",
      "weights range: (-0.3804, 0.5623)\n",
      "layer quant RMSE: 6.2330e-02\n",
      "quantize for: layer3.0.conv1.weight, size: torch.Size([64, 32, 3, 3])\n",
      "weights range: (-0.4159, 0.5689)\n",
      "layer quant RMSE: 6.7724e-02\n",
      "quantize for: layer3.0.conv2.weight, size: torch.Size([64, 64, 3, 3])\n",
      "weights range: (-0.3876, 0.5378)\n",
      "layer quant RMSE: 6.5336e-02\n",
      "quantize for: layer3.1.conv1.weight, size: torch.Size([64, 64, 3, 3])\n",
      "weights range: (-0.3994, 0.5349)\n",
      "layer quant RMSE: 7.3638e-02\n",
      "quantize for: layer3.1.conv2.weight, size: torch.Size([64, 64, 3, 3])\n",
      "weights range: (-0.4060, 0.6366)\n",
      "layer quant RMSE: 6.9443e-02\n",
      "quantize for: layer3.2.conv1.weight, size: torch.Size([64, 64, 3, 3])\n",
      "weights range: (-0.4262, 0.5804)\n",
      "layer quant RMSE: 7.0438e-02\n",
      "quantize for: layer3.2.conv2.weight, size: torch.Size([64, 64, 3, 3])\n",
      "weights range: (-0.2922, 0.3317)\n",
      "layer quant RMSE: 3.7721e-02\n",
      "quantize for: fc.weight, size: torch.Size([10, 64])\n",
      "weights range: (-1.1669, 1.6751)\n",
      "layer quant RMSE: 2.7937e-01\n",
      "\n",
      "total quant RMSE: 7.1134e-02\n"
     ]
    }
   ],
   "source": [
    "# quantize weights\n",
    "rmse = 0\n",
    "checkpoint, rmse = quant_checkpoint(checkpoint)\n",
    "# load the updated weights\n",
    "quantized_model2.load_state_dict(checkpoint)\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3e17e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model2 = quant_model_acts(quantized_model2, CONFIG['bits'], False, CONFIG['num_calibration_batches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7f04700",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model2 = load_model_act_stats(quantized_model2, act_stats_save_path, 'top_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c934b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])),\n",
    "        batch_size=CONFIG['valid_batch_size'], shuffle=False,\n",
    "        num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97c41983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "100b7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_inference(model, dataloader, device):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    start = time.time()\n",
    "        \n",
    "    val_loss, val_score, preds = valid_fn(model, dataloader, CONFIG['device'], CONFIG['valid_batch_size'])\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    history['Valid Loss'].append(val_loss)\n",
    "    history['Valid Score'].append(val_score)\n",
    "    \n",
    "    time_elapsed = end - start\n",
    "    print('Validation complete in {:.0f}ms'.format(\n",
    "        time_elapsed * 1000))\n",
    "    print(\"Validation Loss: {:.4f}\".format(val_loss))\n",
    "    print(\"Validation Score: {:.4f}\".format(val_score))\n",
    "    \n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1776f4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model after quantization\n",
      "Size (MB): 1.232297\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(quantized_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aff19ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 2080\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 40/40 [00:08<00:00,  4.76it/s, Valid_Loss=13.5, Valid_Score=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation complete in 8522ms\n",
      "Validation Loss: 13.4887\n",
      "Validation Score: 0.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "quantized_model2, history = performance_inference(quantized_model2, validation_loader, CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4082dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
